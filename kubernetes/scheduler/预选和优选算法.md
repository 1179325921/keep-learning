## 前言
kube-scheduler调度器默认提供了两类调度算法，分别是预选调度算法和优选调度算法。
- 预选调度算法: 检查节点是否符合运行“带调度的Pod资源对象”的条件，如果符合条件，则将其加入可用节点列表
- 优选调度算法: 为每个可用节点计算出一个最终分数，调度器会选择得分最高的节点

## 预选调度算法
每一个预选调度算法都需要实现FitPredicate的type func,FitPredicate接收一个带调度的Pod资源对象(即 pod *v1.Pod)和一个带选择节点(即nodeInfor *schedulernodeinfo.NodeInfo),通过预选调度算法计算待选择节点是否符合运行。

代码路径`pkg/scheduler/algorithm/predicates/predicates.go:154`
```go
type FitPredicate func(pod *v1.Pod, meta PredicateMetadata, nodeInfo *schedulernodeinfo.NodeInfo) (bool, []PredicateFailureReason, error)
```
在执行预选调度算法后，如果符合条件，则返回True,如果不符合条件，则返回false,并将预选失败原因记录到[]PredicateFailureReason中

预选调度算法内容如下:
```
var (
	predicatesOrdering = []string{CheckNodeConditionPred, CheckNodeUnschedulablePred,
		GeneralPred, HostNamePred, PodFitsHostPortsPred,
		MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred,
		PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred,
		CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred,
		MaxAzureDiskVolumeCountPred, MaxCinderVolumeCountPred, CheckVolumeBindingPred, NoVolumeZoneConflictPred,
		CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, MatchInterPodAffinityPred}
)
```
### 预选调度前的优化
当集群规模庞大时，例如拥有5000个节点，调度器没调度一个Pod资源对象都要尝试5000次节点预选过程，这个是非常消耗调度器资源的。
所以，kube-scheduler通过PercentageOfNodesToScore机制进行优化。该机制原理：一旦发现一定数量的可用节点(占所有节点的百分比)，调度器就停止寻找更多的节点。

PercentageOfNodesToScore参数至是一个集群中所有节点的百分比，范围是1～100之间。如果超过100按100来计算，0代表启用该功能，默认值是50

代码路径`pkg/scheduler/scheduler.go：277`->`pkg/scheduler/core/generic_scheduler.go:162`->`pkg/scheduler/core/generic_scheduler.go:435`
```
func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) {
	var filtered []*v1.Node
	failedPredicateMap := FailedPredicateMap{}

	if len(g.predicates) == 0 {
		filtered = nodes
	} else {
		allNodes := int32(g.cache.NodeTree().NumNodes())
		numNodesToFind := g.numFeasibleNodesToFind(allNodes)

		...
	}
	...
}
func (g *genericScheduler) numFeasibleNodesToFind(numAllNodes int32) (numNodes int32) {
	if numAllNodes < minFeasibleNodesToFind || g.percentageOfNodesToScore >= 100 {
		return numAllNodes
	}

	adaptivePercentage := g.percentageOfNodesToScore
	if adaptivePercentage <= 0 {
		adaptivePercentage = schedulerapi.DefaultPercentageOfNodesToScore - numAllNodes/125
		if adaptivePercentage < minFeasibleNodesPercentageToFind {
			adaptivePercentage = minFeasibleNodesPercentageToFind
		}
	}

	numNodes = numAllNodes * adaptivePercentage / 100
	if numNodes < minFeasibleNodesToFind {
		return minFeasibleNodesToFind
	}

	return numNodes
}
```
numFeasibleNodesToFind函数按照percentageOfNodesToScore参数值计算出可参与预选调度的节点数量。

### 预选调度过程
获得了需要参与的预选节点后，遍历这些节点并执行所有的预选调度算法

代码路径`pkg/scheduler/core/generic_scheduler.go:459`
```
func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) {
	...
		checkNode := func(i int) {
			nodeName := g.cache.NodeTree().Next()
			fits, failedPredicates, err := podFitsOnNode(
				pod,
				meta,
				g.nodeInfoSnapshot.NodeInfoMap[nodeName],
				g.predicates,
				g.schedulingQueue,
				g.alwaysCheckAllPredicates,
			)
			if err != nil {
				predicateResultLock.Lock()
				errs[err.Error()]++
				predicateResultLock.Unlock()
				return
			}
			if fits {
				length := atomic.AddInt32(&filteredLen, 1)
				if length > numNodesToFind {
					cancel()
					atomic.AddInt32(&filteredLen, -1)
				} else {
					filtered[length-1] = g.nodeInfoSnapshot.NodeInfoMap[nodeName].Node()
				}
			} else {
				predicateResultLock.Lock()
				failedPredicateMap[nodeName] = failedPredicates
				predicateResultLock.Unlock()
			}
		}

		// Stops searching for more nodes once the configured number of feasible nodes
		// are found.
		workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode)
		...
	return filtered, failedPredicateMap, nil
}
```
通过workqueue.ParallelizeUntil并发执行checkNode函数，默认启用16个goroutine并发的匹配节点。一旦从可用节点数找到通过，就停止搜索更多的可用节点，内部之心cancel函数退出

checkNode函数用于检测节点是否符合运行pod资源对象的条件。内部通过g.cache.NodeTree().Next函数不断的从缓存中获取下一个节点，通过podFitsOnNode函数执行所有的预算调度算法，检查当前节点是否符合条件。
如果符合，则将节点加入filtered数组，并为下一次优选做准备;如果不符合，则将节点加入failedPredicateMap数组，并记录失败的原因

podFitsOnNode函数，它用于运行u所有的预选调度算法来对节点进行计算。在某种情况下，所有的预选调度算法都会执行两遍，这个问题后面详解

代码路径`pkg/scheduler/core/generic_scheduler.go：571`
```go
func podFitsOnNode(
	pod *v1.Pod,
	meta predicates.PredicateMetadata,
	info *schedulernodeinfo.NodeInfo,
	predicateFuncs map[string]predicates.FitPredicate,
	queue internalqueue.SchedulingQueue,
	alwaysCheckAllPredicates bool,
) (bool, []predicates.PredicateFailureReason, error) {
	var failedPredicates []predicates.PredicateFailureReason

	podsAdded := false
	// We run predicates twice in some cases. If the node has greater or equal priority
	// nominated pods, we run them when those pods are added to meta and nodeInfo.
	// If all predicates succeed in this pass, we run them again when these
	// nominated pods are not added. This second pass is necessary because some
	// predicates such as inter-pod affinity may not pass without the nominated pods.
	// If there are no nominated pods for the node or if the first run of the
	// predicates fail, we don't run the second pass.
	// We consider only equal or higher priority pods in the first pass, because
	// those are the current "pod" must yield to them and not take a space opened
	// for running them. It is ok if the current "pod" take resources freed for
	// lower priority pods.
	// Requiring that the new pod is schedulable in both circumstances ensures that
	// we are making a conservative decision: predicates like resources and inter-pod
	// anti-affinity are more likely to fail when the nominated pods are treated
	// as running, while predicates like pod affinity are more likely to fail when
	// the nominated pods are treated as not running. We can't just assume the
	// nominated pods are running because they are not running right now and in fact,
	// they may end up getting scheduled to a different node.
	for i := 0; i < 2; i++ {
		metaToUse := meta
		nodeInfoToUse := info
		if i == 0 {
			podsAdded, metaToUse, nodeInfoToUse = addNominatedPods(pod, meta, info, queue)
		} else if !podsAdded || len(failedPredicates) != 0 {
			break
		}
		for _, predicateKey := range predicates.Ordering() {
			var (
				fit     bool
				reasons []predicates.PredicateFailureReason
				err     error
			)
			//TODO (yastij) : compute average predicate restrictiveness to export it as Prometheus metric
			if predicate, exist := predicateFuncs[predicateKey]; exist {
				fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse)
				if err != nil {
					return false, []predicates.PredicateFailureReason{}, err
				}

				if !fit {
					// eCache is available and valid, and predicates result is unfit, record the fail reasons
					failedPredicates = append(failedPredicates, reasons...)
					// if alwaysCheckAllPredicates is false, short circuit all predicates when one predicate fails.
					if !alwaysCheckAllPredicates {
						klog.V(5).Infoln("since alwaysCheckAllPredicates has not been set, the predicate " +
							"evaluation is short circuited and there are chances " +
							"of other predicates failing as well.")
						break
					}
				}
			}
		}
	}

	return len(failedPredicates) == 0, failedPredicates, nil
}
```
predicate.Ordering()包含了所有预选调度算法，预选调度算法默认是经过排序的，执行时会按照顺序执行。执行已注册的预选调度算法对节点进行计算，当所有调度算法对节点返回true时，则节点完成预选。如果返回false,则加入failedPredicates数组

以CheckNodeConditionPred预选调度算法为例，代码路径`pkg/scheduler/algorithm/predicates/predicates.go:1609`
```go
func CheckNodeConditionPredicate(pod *v1.Pod, meta PredicateMetadata, nodeInfo *schedulernodeinfo.NodeInfo) (bool, []PredicateFailureReason, error) {
	reasons := []PredicateFailureReason{}
	if nodeInfo == nil || nodeInfo.Node() == nil {
		return false, []PredicateFailureReason{ErrNodeUnknownCondition}, nil
	}

	node := nodeInfo.Node()
	for _, cond := range node.Status.Conditions {
		// We consider the node for scheduling only when its:
		// - NodeReady condition status is ConditionTrue,
		// - NodeNetworkUnavailable condition status is ConditionFalse.
		if cond.Type == v1.NodeReady && cond.Status != v1.ConditionTrue {
			reasons = append(reasons, ErrNodeNotReady)
		} else if cond.Type == v1.NodeNetworkUnavailable && cond.Status != v1.ConditionFalse {
			reasons = append(reasons, ErrNodeNetworkUnavailable)
		}
	}

	if node.Spec.Unschedulable {
		reasons = append(reasons, ErrNodeUnschedulable)
	}

	return len(reasons) == 0, reasons, nil
}
```
Node资源对象的Status.Conditions字段用于描述节点状态，节点状态可以是NotReaady,NodeOutOfDisk,NodeMemoryPressure,NodeDiskPressure,NodePIDPressure,NodeNetworkUnavailable等。其中如果NodeReady为true,则表示节点运行状态良好并可以被调度

CheckNodeConditionPredicate算法会检查节点是否处于就绪状态，该算法返回后继续执行下一个预选调度算法。

回到podFitsOnNode函数，之前提到会执行两次(for i:=0;i<2;i++{...})
```
for i := 0; i < 2; i++ {
		metaToUse := meta
		nodeInfoToUse := info
		if i == 0 {
			podsAdded, metaToUse, nodeInfoToUse = addNominatedPods(pod, meta, info, queue)
		} else if !podsAdded || len(failedPredicates) != 0 {
			break
		}
		// 预选算法逻辑
	}
```
计算两次这与pod资源的亲和性有关。

addNominatedPods函数从调度队列中找到节点上优先级大于或等于当前Pod资源对象的NominatedPods，如果节点上存在NominatedPods，则将当前Pod资源对象和NominatedPods加入相同的nodeInfo对象中。其中，NominatedPods表示已经分配到节点上但还没有真正运行起来的Pod资源对象，它们呢咋其他POd资源对象从节点中被删除后才可以进行实际的调度

假设当前调度Pod对象的亲和性策略依赖的是NominatedPods，此时问题就来了，NominatedPods不能保证一定可以调度到对应的节点上。例如，在抢占机制中，NominatedPods有可能被从调度队列中清理

## 优选调度算法
优选调度算法实现是基于Map-Reduce的方式对节点进行计算分数，Map-Reduce的方式分为两步:
- Priority Map: 计算节点分数
- Priority Reduce: 汇总每个节点的分数并根据权重计算节点的最终分数

每一种优选调度算法都实现了Map-Reduce函数,代码路径`pkg/scheduler/algorithm/priorities/types.go：28`
```go
type PriorityMapFunction func(pod *v1.Pod, meta interface{}, nodeInfo *schedulernodeinfo.NodeInfo) (schedulerapi.HostPriority, error)

type PriorityReduceFunction func(pod *v1.Pod, meta interface{}, nodeNameToInfo map[string]*schedulernodeinfo.NodeInfo, result schedulerapi.HostPriorityList) error

```
针对没一个可用节点，优选调度算法都会为其计算出一个0-10之间的分数，表示Pod资源对象调度到该节点上的合适度，10表示非常适合，0表示不太适合。该过程通过Priority Map进行计算。

每一种优选调度算法都有一个权重值(权重因子)，他用于计算节点的最终分数。最终分数通过计算权重值和优选调度算法的乘积获得。例如，假如有两个优选调度算法，分别是priorityFunc1和priorityFunc2，它们分别拥有权重值weight1和weight2,那么某节点的最终分数的公式如下“
```
findlScoreNodeA = (weight1*priorityFunc1)+(weight2*priroityFunc2)
```
在计算完所有节点的分数后，kube-scheduler会将分数最高的节点最为运行Pod资源对象的节点。如果有多个节点都获得了最高分，则通过round-robin轮循方式选择一个最佳

提示：所有节点都通过Map-Reduce的方式计算分数，但并非所有的优选调度算法都实现了Reduce,他们可以为nil,例如LeastRequestedPriority

### 优选调度过程
优选调度算法为每一个可用节点计算出一个最终分数。每一个节点的分数可通过HostPriorityList进行描述

代码路径`pkg/scheduler/api/types.go:331`
```go
type HostPriority struct {
	// Name of the host
	Host string
	// Score associated with the host
	Score int
}
type HostPriorityList []HostPriority
```
优选过程通过PriorityNodes函数完成，每个优选算法都通过Map-Reduce方式计算分数。

#### Priority Map计算节点分数
代码路径`pkg/scheduler/core/generic_scheduler.go：645`
```go
func PrioritizeNodes(
	pod *v1.Pod,
	nodeNameToInfo map[string]*schedulernodeinfo.NodeInfo,
	meta interface{},
	priorityConfigs []priorities.PriorityConfig,
	nodes []*v1.Node,
	extenders []algorithm.SchedulerExtender,
) (schedulerapi.HostPriorityList, error) {
	...
	results := make([]schedulerapi.HostPriorityList, len(priorityConfigs), len(priorityConfigs))

	// DEPRECATED: we can remove this when all priorityConfigs implement the
	// Map-Reduce pattern.
	for i := range priorityConfigs {
		if priorityConfigs[i].Function != nil {
			wg.Add(1)
			go func(index int) {
				defer wg.Done()
				var err error
				results[index], err = priorityConfigs[index].Function(pod, nodeNameToInfo, nodes)
				if err != nil {
					appendError(err)
				}
			}(i)
		} else {
			results[i] = make(schedulerapi.HostPriorityList, len(nodes))
		}
	}

	workqueue.ParallelizeUntil(context.TODO(), 16, len(nodes), func(index int) {
		nodeInfo := nodeNameToInfo[nodes[index].Name]
		for i := range priorityConfigs {
			if priorityConfigs[i].Function != nil {
				continue
			}

			var err error
			results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo)
			if err != nil {
				appendError(err)
				results[i][index].Host = nodes[index].Name
			}
		}
	})

	...
}
```
优选调度算法的Priority Map过程跟预选调度过程类似，它通过workqueue.ParallelizeUntil并发执行匿名函数，默认启用16个goroutine。内部通过nodeNameToInfo数组不断的获取下一个节点，通过已注册的priorityConfigs[i].Map优选算法对每个节点执行计算过程，将分数存储到results。如果err记录异常的原因

#### Priority Reduce
汇总每个节点的分数并更及权重计算节点的最终分数

代码路径``
```go
// ... 上面Map逻辑
for i := range priorityConfigs {
		if priorityConfigs[i].Reduce == nil {
			continue
		}
		wg.Add(1)
		go func(index int) {
			defer wg.Done()
			if err := priorityConfigs[index].Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil {
				appendError(err)
			}
			if klog.V(10) {
				for _, hostPriority := range results[index] {
					klog.Infof("%v -> %v: %v, Score: (%d)", util.GetPodFullName(pod), hostPriority.Host, priorityConfigs[index].Name, hostPriority.Score)
				}
			}
		}(i)
	}
	// Wait for all computations to be finished.
	wg.Wait()
	if len(errs) != 0 {
		return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs)
	}

	// Summarize all scores.
	result := make(schedulerapi.HostPriorityList, 0, len(nodes))

	for i := range nodes {
		result = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0})
		for j := range priorityConfigs {
			result[i].Score += results[j][i].Score * priorityConfigs[j].Weight
		}
	}
...
```
Priority Reduce也是通过并发来实现计算过程。通过已注册的priorityConfigs[index].Reduce对每个节点分数进行汇总，通过wg.Wait()等待计算结果。

最后，更具权重计算最终分数。(result[i].Score += results[j][i].Score * priorityConfigs[j].Weight)

以LeastRequestedPriority为例，代码如下

代码路径`pkg/scheduler/algorithm/priorities/least_requested.go:36`
```go
func leastResourceScorer(requested, allocable *schedulernodeinfo.Resource, includeVolumes bool, requestedVolumes int, allocatableVolumes int) int64 {
	return (leastRequestedScore(requested.MilliCPU, allocable.MilliCPU) +
		leastRequestedScore(requested.Memory, allocable.Memory)) / 2
}

// The unused capacity is calculated on a scale of 0-10
// 0 being the lowest priority and 10 being the highest.
// The more unused resources the higher the score is.
func leastRequestedScore(requested, capacity int64) int64 {
	if capacity == 0 {
		return 0
	}
	if requested > capacity {
		return 0
	}

	return ((capacity - requested) * int64(schedulerapi.MaxPriority)) / capacity
}
```
LeastRequestedPriority会计算Pod资源对象所需的CPU和内存占当前节点可用资源的百分比，百分比最小的节点最优，他的默认权重1。计算公式为
```
(cpu((capacity-sum(requested))*10/capacity)+ memory((capacity-sum(requested))*10/capacity))/2

即(cpu(总资源-已使用资源)*10/总资源+memory(总资源-已使用资源)*10/总资源)/2
```

## 选择一个最佳节点
在计算完所有节点分数后，kube-scheduler调度器会通过selectHost函数将分数最高的节点作为运行Pod的节点。如果多个节点得了最高分，则round-robin轮循选择一个最佳节点。代码如下

代码路径`pkg/scheduler/core/generic_scheduler.go：264`
```go
func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) {
	if len(priorityList) == 0 {
		return "", fmt.Errorf("empty priorityList")
	}

	maxScores := findMaxScores(priorityList)
	ix := int(g.lastNodeIndex % uint64(len(maxScores)))
	g.lastNodeIndex++

	return priorityList[maxScores[ix]].Host, nil
}
```
findMaxScores函数得到priorityList中具有最高分的节点索引列表，如果有多个，则跟据个数进行轮循选择(int(g.lastNodeIndex % uint64(len(maxScores))))